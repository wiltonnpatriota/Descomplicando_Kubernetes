apiVersion: v1
kind: ConfigMap
metadata:
  name: kubernetes-deploy-sexta-feira-lab
  namespace: girus
  labels:
    app: girus-lab-template
data:
  lab.yaml: |
    name: kubernetes-deploy-sexta-feira
    title: "Caso 1: O Deploy de Sexta-Feira"
    description: "Sexta-feira, 4 da tarde. Uma altera√ß√£o 'simples' derrubou a produ√ß√£o. Neste laborat√≥rio interativo, voc√™ vai vivenciar um cen√°rio real de troubleshooting no Kubernetes, aprendendo a diagnosticar problemas de OOMKilled, Probes falhando e estrat√©gias de rollout. Prepare-se para resolver o mist√©rio do deploy que arruinou o fim de semana!"
    duration: 30m
    image: "linuxtips/girus-kind-single-node:0.1"
    privileged: true
    tasks:
      - name: "O Cen√°rio: '√â S√≥ Uma Linha de C√≥digo'"
        description: "Simule o deploy fat√≠dico de sexta-feira e observe o caos se instalar no cluster."
        steps:
          - "**Sexta-feira, 4 da tarde.**"
          - "O time de produto pediu uma altera√ß√£o pequena, quase cosm√©tica, em um dos microsservi√ßos. '√â s√≥ mudar um texto, super r√°pido', eles disseram."
          - "Voc√™ fez a altera√ß√£o, rodou os testes locais, tudo verde. Parece seguro."
          - "Vamos criar o namespace para nossa simula√ß√£o:"
          - "`kubectl create namespace producao`"
          - "Agora, vamos simular o cen√°rio inicial: um Deployment **sem boas pr√°ticas** que estava funcionando 'na sorte'. Crie o arquivo `deployment-problematico.yaml`:"
          - |
            ```yaml
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: meu-servico
              namespace: producao
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: meu-servico
              template:
                metadata:
                  labels:
                    app: meu-servico
                spec:
                  containers:
                  - name: meu-servico-container
                    image: nginx:alpine
                    # PROBLEMA 1: Sem resources (requests/limits)
                    # PROBLEMA 2: Sem probes (liveness/readiness/startup)
                    # PROBLEMA 3: Deployment criado h√° muito tempo, sem seguir boas pr√°ticas
            ```
          - "Aplique o manifesto problem√°tico:"
          - "`kubectl apply -f deployment-problematico.yaml`"
          - "Aguarde os pods ficarem prontos:"
          - "`kubectl get pods -n producao -w`"
          - "Pressione Ctrl+C quando os 3 pods estiverem Running."
          - "Tudo parece bem... por enquanto. Voc√™ abre o terminal, confiante, e se prepara para o 'pequeno deploy'."
          - "**Mas ent√£o, seu celular vibra. E vibra de novo. √â o PagerDuty.**"
          - "O canal de alertas do time come√ßa a piscar em vermelho no Slack. O dashboard do Grafana, antes um mar de tranquilidade verde, agora parece uma √°rvore de natal desgovernada."
          - "Seu colega manda uma mensagem: *'Cara, o servi√ßo X caiu depois do seu deploy'*."
          - "Aquele caf√© que voc√™ ia tomar vai ter que esperar."
        tips:
          - type: "warning"
            title: "D√≠vida T√©cnica √† Vista"
            content: "Este Deployment foi criado sem as garantias m√≠nimas de estabilidade que o Kubernetes oferece. Sem resources e probes, a aplica√ß√£o est√° rodando 'na sorte'."
          - type: "info"
            title: "O que aconteceu?"
            content: "Como uma mudan√ßa de uma linha de texto derrubou um servi√ßo inteiro? Vamos descobrir seguindo um roteiro l√≥gico de troubleshooting."
        validation:
          - command: "kubectl get deployment meu-servico -n producao -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo '0'"
            expectedOutput: "3"
            errorMessage: "O Deployment inicial n√£o est√° com 3 r√©plicas prontas. Verifique se aplicou o manifesto corretamente."

      - name: "Passo 1: Qual o Estado da Aplica√ß√£o?"
        description: "Respira fundo e come√ßa do b√°sico. O primeiro comando √© sempre kubectl get pods - nossa vis√£o geral do campo de batalha."
        steps:
          - "**Nessas horas, o p√¢nico n√£o ajuda.**"
          - "A primeira regra de um bom profissional SRE ou Dev √©: **respira fundo e come√ßa do b√°sico**. N√£o adianta chutar. Precisamos de dados."
          - "Antes de investigar, vamos simular o problema. Crie uma nova vers√£o 'quebrada' do deployment que consome muita mem√≥ria. Crie o arquivo `deployment-quebrado.yaml`:"
          - |
            ```yaml
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: meu-servico
              namespace: producao
            spec:
              replicas: 3
              selector:
                matchLabels:
                  app: meu-servico
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxUnavailable: 1
                  maxSurge: 1
              template:
                metadata:
                  labels:
                    app: meu-servico
                    version: "v2-problematica"
                spec:
                  containers:
                  - name: meu-servico-container
                    image: linuxtips/memory-eater:0.1
                    # Simulando limites muito baixos para for√ßar OOMKill
                    resources:
                      limits:
                        memory: "32Mi"
                    # Sem probes = Kubernetes operando √†s cegas
            ```
          - "Aplique o deploy problem√°tico:"
          - "`kubectl apply -f deployment-quebrado.yaml`"
          - "Agora, observe o campo de batalha com o comando mais importante:"
          - "`kubectl get pods -n producao`"
          - "Voc√™ provavelmente ver√° algo assim:"
          - "- Alguns pods da vers√£o antiga ainda no ar (Running)"
          - "- Pods da nova vers√£o em **CrashLoopBackOff** ou **OOMKilled**"
          - "- Talvez algum Pod em **Pending**"
          - "Aguarde alguns segundos e observe novamente:"
          - "`kubectl get pods -n producao -w`"
          - "Pressione Ctrl+C ap√≥s observar o comportamento."
          - "**Diagn√≥stico inicial:** O problema est√° claramente na nova vers√£o. Os pods antigos funcionam, os novos n√£o."
        tips:
          - type: "info"
            title: "kubectl get pods - Seu Primeiro Contato"
            content: "Este comando √© sempre o primeiro passo. Ele mostra o estado geral dos seus Pods: Running, Pending, CrashLoopBackOff, Error, etc. √â a 'vis√£o do campo de batalha'."
          - type: "tip"
            title: "Identificando Vers√µes pelos Hashes"
            content: "O nome do Pod inclui um hash do ReplicaSet (ex: meu-servico-5f... vs meu-servico-7d...). Pods com hashes diferentes pertencem a vers√µes/ReplicaSets diferentes."
          - type: "warning"
            title: "CrashLoopBackOff"
            content: "Este status indica que o container est√° reiniciando repetidamente. O Kubernetes aumenta o tempo entre as tentativas (backoff) para n√£o sobrecarregar o sistema."
        validation:
          - command: "kubectl get pods -n producao 2>/dev/null | grep -E 'CrashLoopBackOff|OOMKilled|Error' | wc -l | tr -d ' '"
            expectedExpression: ">= 1"
            errorMessage: "Nenhum Pod problem√°tico encontrado. Aplique o deployment-quebrado.yaml e aguarde alguns segundos."

      - name: "Passo 2: Aprofundando nos Logs e Eventos"
        description: "Agora que temos um suspeito, vamos interrog√°-lo. O kubectl describe pod √© nossa principal ferramenta de investiga√ß√£o."
        steps:
          - "**Hora de interrogar o suspeito!**"
          - "Primeiro, vamos pegar o nome de um dos Pods problem√°ticos:"
          - "`POD_PROBLEMA=$(kubectl get pods -n producao -l version=v2-problematica -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)`"
          - "Agora, usamos o **kubectl describe** para ver o hist√≥rico completo do Pod:"
          - "`kubectl describe pod $POD_PROBLEMA -n producao`"
          - "**O que procurar na sa√≠da:**"
          - "1. **Se√ß√£o 'State' e 'Last State'**: Mostra o estado atual e o anterior do container"
          - "2. **Reason**: Procure por `OOMKilled` - significa que o processo foi morto por excesso de mem√≥ria"
          - "3. **Exit Code**: C√≥digo diferente de 0 indica falha"
          - "4. **Se√ß√£o 'Events'**: A linha do tempo do crime - mostra o hist√≥rico de eventos"
          - "Vamos filtrar especificamente os eventos:"
          - "`kubectl describe pod $POD_PROBLEMA -n producao | grep -A 20 'Events:'`"
          - "Voc√™ deve ver eventos como:"
          - "- `Warning Unhealthy` - Readiness probe failed"
          - "- `Warning BackOff` - Back-off restarting failed container"
          - "- `Normal Killing` - Container sendo morto"
          - "Vamos verificar os logs do container (se houver):"
          - "`kubectl logs $POD_PROBLEMA -n producao --previous 2>/dev/null || echo 'Sem logs anteriores dispon√≠veis'`"
          - "**Descoberta:** A aplica√ß√£o est√° consumindo mais mem√≥ria do que o permitido (OOMKilled) e n√£o h√° probes configuradas para o Kubernetes saber se ela est√° saud√°vel!"
        tips:
          - type: "info"
            title: "OOMKilled - Out of Memory Killed"
            content: "Quando um container excede o limite de mem√≥ria definido, o Kernel do Linux o mata imediatamente. √â uma prote√ß√£o do sistema para evitar que um processo consuma toda a mem√≥ria do Node."
          - type: "warning"
            title: "Exit Code 137"
            content: "O c√≥digo de sa√≠da 137 (128 + 9) indica que o processo recebeu um SIGKILL. Isso geralmente significa OOMKilled ou o container foi for√ßosamente terminado."
          - type: "tip"
            title: "Flag --previous"
            content: "Use `kubectl logs --previous` para ver os logs da execu√ß√£o anterior do container. Essencial quando o container atual est√° reiniciando constantemente."
        validation:
          - command: "kubectl describe pod -n producao -l version=v2-problematica 2>/dev/null | grep -c 'OOMKilled\\|BackOff\\|Unhealthy' || echo '0'"
            expectedExpression: ">= 1"
            errorMessage: "N√£o foi poss√≠vel encontrar evid√™ncias de OOMKilled ou falhas. Verifique se o Pod problem√°tico existe."

      - name: "Passo 3: Como Foi a Transi√ß√£o?"
        description: "Um Deployment n√£o gerencia Pods diretamente - ele usa ReplicaSets. Entender o que aconteceu com eles nos ajuda a entender o rollout."
        steps:
          - "**Entendendo a Hierarquia: Deployment ‚Üí ReplicaSet ‚Üí Pods**"
          - "Um conceito fundamental: cada vez que voc√™ faz um deploy, um **novo ReplicaSet** √© criado."
          - "Vamos ver todos os ReplicaSets do nosso Deployment:"
          - "`kubectl get replicaset -n producao`"
          - "Voc√™ deve ver algo como:"
          - "```"
          - "NAME DESIRED CURRENT READY AGE"
          - "meu-servico-5f4d... 2 2 2 10m # Vers√£o antiga"
          - "meu-servico-7d8e... 3 3 0 2m # Vers√£o nova (problem√°tica)"
          - "```"
          - "**An√°lise:**"
          - "- O ReplicaSet antigo ainda tem pods Ready (sobreviventes)"
          - "- O ReplicaSet novo tem pods DESIRED mas 0 READY"
          - "- O Kubernetes est√° tentando criar os novos pods, mas eles morrem"
          - "Vamos ver mais detalhes:"
          - "`kubectl describe replicaset -n producao -l version=v2-problematica`"
          - "Na se√ß√£o Events, voc√™ ver√° o ReplicaSet tentando criar pods repetidamente."
          - "**O ReplicaSet est√° fazendo seu trabalho**. Ele tenta manter o n√∫mero de r√©plicas desejado. O problema √© que os Pods n√£o colaboram - eles morrem assim que sobem."
        tips:
          - type: "info"
            title: "A Hierarquia K8s"
            content: "Deployment gerencia ReplicaSets, que gerenciam Pods. O Deployment cuida das atualiza√ß√µes e rollbacks. O ReplicaSet garante o n√∫mero de r√©plicas. Os Pods executam os containers."
          - type: "tip"
            title: "Por que m√∫ltiplos ReplicaSets?"
            content: "Durante um rolling update, o Kubernetes mant√©m o ReplicaSet antigo (para rollback) enquanto cria o novo. √â por isso que voc√™ v√™ dois ReplicaSets para o mesmo Deployment."
          - type: "warning"
            title: "Rolling Update Travado"
            content: "Se os novos Pods n√£o ficam Ready, o rolling update fica travado. O Kubernetes n√£o vai matar todos os Pods antigos at√© que os novos estejam funcionando."
        validation:
          - command: "kubectl get replicaset -n producao --no-headers 2>/dev/null | wc -l | tr -d ' '"
            expectedExpression: ">= 2"
            errorMessage: "Deve haver pelo menos 2 ReplicaSets (vers√£o antiga e nova). Verifique se o deploy foi aplicado."

      - name: "Passo 4: Revisando o Manifesto (A Causa Raiz)"
        description: "Se os Pods est√£o com problemas, a causa raiz est√° na especifica√ß√£o - o 'DNA' deles. Vamos examinar o Deployment."
        steps:
          - "**Analisando o DNA do Problema**"
          - "Vamos olhar a configura√ß√£o completa do Deployment:"
          - "`kubectl describe deployment meu-servico -n producao`"
          - "Agora, vamos extrair especificamente a especifica√ß√£o do container:"
          - "`kubectl get deployment meu-servico -n producao -o yaml | grep -A 30 'containers:'`"
          - "**Os Problemas Encontrados:**"
          - "1. **Falta de Resources adequados**: O limite de mem√≥ria est√° muito baixo (32Mi), causando OOMKill"
          - "2. **Aus√™ncia de Probes**: Sem liveness, readiness ou startup probes"
          - "3. **Sem requests definidos**: O Scheduler n√£o sabe quanto recurso a aplica√ß√£o precisa"
          - "Vamos confirmar a aus√™ncia de probes:"
          - "`kubectl get deployment meu-servico -n producao -o jsonpath='{.spec.template.spec.containers[0].livenessProbe}' && echo ' <- Liveness' || echo 'Sem Liveness Probe'`"
          - "`kubectl get deployment meu-servico -n producao -o jsonpath='{.spec.template.spec.containers[0].readinessProbe}' && echo ' <- Readiness' || echo 'Sem Readiness Probe'`"
          - "**A Verdade:** Este servi√ßo foi criado h√° muito tempo, provavelmente por algu√©m que n√£o seguiu as melhores pr√°ticas. Ele sempre funcionou 'na sorte'. A pequena mudan√ßa de c√≥digo aumentou o consumo de mem√≥ria, e foi **a gota d'√°gua** que fez o castelo de cartas desmoronar."
        tips:
          - type: "warning"
            title: "D√≠vida T√©cnica que Venceu"
            content: "O problema n√£o foi a sua linha de c√≥digo. Foi uma d√≠vida t√©cnica esperando para ser cobrada. A aplica√ß√£o estava rodando sem as garantias m√≠nimas de estabilidade."
          - type: "info"
            title: "A Combina√ß√£o Fatal"
            content: "1) Sem requests/limits = bomba-rel√≥gio de mem√≥ria. 2) Sem probes = Kubernetes operando √†s cegas. 3) RollingUpdate sem readiness = caos durante deploys."
          - type: "tip"
            title: "Preven√ß√£o"
            content: "Use ferramentas como OPA Gatekeeper ou Kyverno para bloquear Deployments sem resources e probes em produ√ß√£o."
        validation:
          - command: "kubectl get deployment meu-servico -n producao -o jsonpath='{.spec.template.spec.containers[0].resources.limits.memory}' 2>/dev/null"
            expectedOutput: "32Mi"
            errorMessage: "Verifique se o Deployment problem√°tico est√° aplicado com o limite de mem√≥ria baixo."

      - name: "A Solu√ß√£o: O Hotfix Salvador"
        description: "Hora de corrigir o problema da forma certa, aplicando todo o conhecimento de resources, probes e estrat√©gias de rollout."
        steps:
          - "**Hora de Corrigir o Problema da Forma Certa!**"
          - "Voc√™ cria um novo branch: `hotfix/add-resources-and-probes`"
          - "Crie o arquivo `deployment-corrigido.yaml` com um manifesto robusto:"
          - |
            ```yaml
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: meu-servico
              namespace: producao
            spec:
              replicas: 3
              strategy:
                type: RollingUpdate
                rollingUpdate:
                  maxUnavailable: 1 # No m√°ximo 1 pod indispon√≠vel
                  maxSurge: 1 # No m√°ximo 1 pod extra durante update
              selector:
                matchLabels:
                  app: meu-servico
              template:
                metadata:
                  labels:
                    app: meu-servico
                    version: "v3-corrigida"
                spec:
                  containers:
                  - name: meu-servico-container
                    image: nginx:alpine
                    ports:
                    - containerPort: 80
                    # CORRE√á√ÉO 1: Definindo uma "cota" de recursos justa
                    resources:
                      requests:
                        cpu: "100m"
                        memory: "128Mi"
                      limits:
                        cpu: "200m"
                        memory: "256Mi"
                    # CORRE√á√ÉO 2: Dando tempo para a aplica√ß√£o iniciar
                    startupProbe:
                      httpGet:
                        path: /
                        port: 80
                      failureThreshold: 30
                      periodSeconds: 10
                    # CORRE√á√ÉO 3: Checando se a aplica√ß√£o n√£o travou
                    livenessProbe:
                      httpGet:
                        path: /
                        port: 80
                      initialDelaySeconds: 10
                      periodSeconds: 10
                    # CORRE√á√ÉO 4: Verificando se est√° pronta para o trabalho
                    readinessProbe:
                      httpGet:
                        path: /
                        port: 80
                      initialDelaySeconds: 5
                      periodSeconds: 5
            ```
          - "Aplique o hotfix:"
          - "`kubectl apply -f deployment-corrigido.yaml`"
          - "Desta vez, observe o rollout de forma controlada:"
          - "`kubectl rollout status deployment/meu-servico -n producao`"
          - "Verifique os pods subindo um a um, de forma ordenada:"
          - "`kubectl get pods -n producao -w`"
          - "Pressione Ctrl+C quando todos os pods estiverem Running e Ready (1/1)."
          - "**O dashboard volta a ficar verde. Voc√™ respira aliviado.**"
        tips:
          - type: "success"
            title: "üéâ Problema Resolvido!"
            content: "Os pods agora t√™m resources adequados, probes configuradas e a estrat√©gia de rollout funciona corretamente porque o Kubernetes sabe quando um pod est√° pronto."
          - type: "info"
            title: "O Papel de Cada Probe"
            content: "startupProbe: D√° tempo para iniciar. livenessProbe: Detecta se travou. readinessProbe: Indica se pode receber tr√°fego. Juntas, garantem alta disponibilidade."
          - type: "tip"
            title: "kubectl rollout status"
            content: "Este comando acompanha o progresso do rollout em tempo real e s√≥ retorna quando o deploy est√° completo ou falhou."
        validation:
          - command: "kubectl get deployment meu-servico -n producao -o jsonpath='{.status.readyReplicas}' 2>/dev/null"
            expectedOutput: "3"
            errorMessage: "O Deployment corrigido n√£o est√° com 3 r√©plicas prontas. Verifique se aplicou o manifesto corrigido."
          - command: "kubectl get deployment meu-servico -n producao -o jsonpath='{.spec.template.spec.containers[0].readinessProbe.httpGet.path}' 2>/dev/null"
            expectedOutput: "/"
            errorMessage: "A readinessProbe n√£o est√° configurada corretamente."

      - name: "Li√ß√µes Aprendidas: O Post-Mortem"
        description: "Na segunda-feira, na reuni√£o de time, voc√™ apresenta o post-mortem. O susto de sexta virou li√ß√£o para todos."
        steps:
          - "**Segunda-feira, Reuni√£o de Time**"
          - "Aquele deploy de sexta-feira se tornou uma li√ß√£o valiosa. Voc√™ apresenta o post-mortem."
          - "Vamos documentar as li√ß√µes. Crie o arquivo `POST-MORTEM.md`:"
          - |
            ```markdown
            # Post-Mortem: O Deploy de Sexta-Feira

            ## Resumo do Incidente
            - **Data:** Sexta-feira, 16h
            - **Dura√ß√£o:** ~2 horas
            - **Impacto:** Servi√ßo parcialmente indispon√≠vel
            - **Causa Raiz:** D√≠vida t√©cnica (falta de resources e probes)

            ## O que aconteceu?
            Uma altera√ß√£o "simples" de c√≥digo aumentou levemente o consumo de mem√≥ria,
            causando OOMKill em Pods que n√£o tinham limites adequados nem probes
            configuradas.

            ## Por que aconteceu?
            1. **Falta de Gest√£o de Recursos (Day-2)**
               - Sem requests e limits, o Pod era uma bomba-rel√≥gio
               - Qualquer varia√ß√£o no consumo ‚Üí OOMKilled

            2. **Estrat√©gia de Rollout Otimista (Day-3)**
               - RollingUpdate sem readiness probes
               - Kubernetes n√£o sabia se o novo Pod estava pronto

            3. **Aus√™ncia de Verifica√ß√µes de Sa√∫de (Day-4)**
               - Sem Readiness Probe ‚Üí tr√°fego para Pods quebrados
               - Sem Liveness Probe ‚Üí Pods "zumbis" no ar

            ## A√ß√µes Corretivas
            - [x] Adicionar resources (requests/limits) em todos os Deployments
            - [x] Implementar startupProbe, livenessProbe e readinessProbe
            - [ ] Implementar pol√≠ticas com OPA/Kyverno para bloquear deploys sem probes
            - [ ] Adicionar alarmes para OOMKill no cluster

            ## Li√ß√µes Aprendidas
            1. Recursos n√£o s√£o opcionais - s√£o a BASE da estabilidade
            2. Deploy seguro = estrat√©gia + probes trabalhando juntos
            3. Sua aplica√ß√£o precisa "conversar" com o Kubernetes via probes
            ```
          - "Agora, vamos verificar que nosso sistema est√° robusto. Confira os recursos:"
          - "`kubectl get deployment meu-servico -n producao -o jsonpath='{.spec.template.spec.containers[0].resources}' | jq .`"
          - "Confira as probes:"
          - "`kubectl get deployment meu-servico -n producao -o jsonpath='{.spec.template.spec.containers[0]}' | jq '{startupProbe, livenessProbe, readinessProbe}'`"
          - "**üéì O que come√ßou como um susto de sexta-feira terminou com um sistema mais robusto e um time mais experiente.**"
          - "E essa √© uma das melhores partes de trabalhar com tecnologia: **Problema resolvido!**"
        tips:
          - type: "success"
            title: "üèÜ Parab√©ns, Detetive!"
            content: "Voc√™ completou a investiga√ß√£o! Agora voc√™ sabe diagnosticar e corrigir problemas de OOMKill, Probes e Rolling Updates no Kubernetes."
          - type: "info"
            title: "Post-Mortems S√£o Essenciais"
            content: "Documentar incidentes sem culpar pessoas (blameless) ajuda todo o time a aprender. O objetivo √© melhorar processos, n√£o apontar dedos."
          - type: "tip"
            title: "Pr√≥ximos Passos"
            content: "Implemente pol√≠ticas de admiss√£o (OPA Gatekeeper ou Kyverno) para bloquear Deployments sem resources e probes em produ√ß√£o. Preven√ß√£o > Corre√ß√£o!"
        validation:
          - command: "kubectl get pods -n producao -l app=meu-servico --no-headers 2>/dev/null | grep -c 'Running' || echo '0'"
            expectedOutput: "3"
            errorMessage: "Nem todos os pods est√£o Running. Verifique o status do Deployment."
          - command: "kubectl get deployment meu-servico -n producao -o jsonpath='{.spec.template.spec.containers[0].livenessProbe.httpGet.path}' 2>/dev/null"
            expectedOutput: "/"
            errorMessage: "A livenessProbe n√£o est√° configurada corretamente no Deployment final."